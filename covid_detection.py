# -*- coding: utf-8 -*-
"""Covid_Detection.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1IKG1SnVoQDJIi0XAMd1w5IYKZBZKYD1Z
"""

!pip install -U segmentation-models

import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
import zipfile
import cv2
from skimage import io
import tensorflow as tf
from tensorflow.python.keras import Sequential
from tensorflow.keras import layers, optimizers
from tensorflow.keras.layers import *
from tensorflow.keras.models import Model, load_model
from tensorflow.keras.initializers import glorot_uniform
from tensorflow.keras.utils import plot_model
from tensorflow.keras.callbacks import ReduceLROnPlateau, ModelCheckpoint, LearningRateScheduler
from IPython.display import display
from tensorflow.keras import backend as K
from sklearn.preprocessing import StandardScaler, normalize
from mpl_toolkits.axes_grid import ImageGrid
from sklearn.model_selection import train_test_split
from keras.preprocessing.image import ImageDataGenerator
from google.colab import files
from random import sample
import os
import glob
import random
import nibabel as nib
from segmentation_models.losses import bce_jaccard_loss
from segmentation_models.metrics import iou_score

from tensorflow.keras.utils import normalize
import segmentation_models as sm

sm.set_framework('tf.keras')
sm.framework()

uploaded = files.upload()

# for fn in uploaded.keys():
#   print('User uploaded file "{name}" with length {length} bytes'.format(
#       name=fn, length=len(uploaded[fn])))

!mkdir -p ~/.kaggle/ && mv kaggle.json ~/.kaggle/ && chmod 600 ~/.kaggle/kaggle.json
!cd /root/.kaggle/ && ls -la

!kaggle datasets download -d andrewmvd/covid19-ct-scans
!unzip /content/covid19-ct-scans

"""# Reading Nil files """

def readingNii(filepath):
  img = nib.load(filepath)
  array   = img.get_fdata()
  array   = np.array(array)
  return(array)

sampleCTScan = readingNii('/content/ct_scans/coronacases_org_001.nii')
sampleLung = readingNii('/content/lung_mask/coronacases_001.nii')
sampleInfection = readingNii('/content/infection_mask/coronacases_001.nii')
sampleAll = readingNii('/content/lung_and_infection_mask/coronacases_001.nii')

fig = plt.figure(figsize = (18,15))
plt.subplot(1,4,1)
num = 189
plt.imshow(sampleCTScan[:, :, num])
plt.title('Original Image')

plt.subplot(1,4,2)
plt.imshow(sampleCTScan[..., num], cmap = 'bone')
plt.imshow(sampleLung[..., num],alpha = 0.5, cmap = 'nipy_spectral')
plt.title('Lung Mask')

plt.subplot(1,4,3)
plt.imshow(sampleCTScan[..., num], cmap = 'bone')
plt.imshow(sampleInfection[..., num], alpha = 0.5, cmap = 'nipy_spectral')
plt.title('Infection Mask')

plt.subplot(1,4,4)
plt.imshow(sampleCTScan[..., num], cmap = 'bone')
plt.imshow(sampleAll[..., num], alpha = 0.5, cmap = 'nipy_spectral')
plt.title('Lung and Infection Mask')
plt.plot()

print(sampleCTScan.shape)

data = pd.read_csv('metadata.csv')
data_size = data.shape[0]

!pwd

lungs = []
infections = []
img_size = 256

for i in range(data_size):
    ct = readingNii('/content/ct_scans/' + data['ct_scan'][i].strip().split('/')[-1])
    infect = readingNii('/content/infection_mask/' + data['infection_mask'][i].strip().split('/')[-1])
    print(ct.shape)

    for ii in range(ct.shape[0]):
        lung_img = cv2.resize(ct[ii], dsize = (img_size, img_size),interpolation = cv2.INTER_AREA).astype('uint8')
        infec_img = cv2.resize(infect[ii],dsize=(img_size, img_size),interpolation = cv2.INTER_AREA).astype('uint8')
        # lungs.append(lung_img[..., np.newaxis])
        # infections.append(infec_img[..., np.newaxis])

        lungs.append(lung_img)
        infections.append(infec_img)


    print("len of lungs: " + str(len(lungs)))

lung = np.array(lungs)
infections = np.array(infections)

print(lung.shape)

num = 2793
plt.imshow(lung[num, ...], cmap='bone')
plt.show()
plt.imshow(infections[num,...], cmap = 'nipy_spectral')
plt.show()

x_train, x_test, y_train, y_test = train_test_split(lungs, infections, test_size = 0.20)

x_train, x_test, y_train, y_test = np.array(x_train), np.array(x_test), np.array(y_train), np.array(y_test)
x_train = x_train[..., np.newaxis]
x_test = x_test[..., np.newaxis]
y_train = y_train[..., np.newaxis]
y_test = y_test[..., np.newaxis]

y_test.shape, y_test.shape, x_train.shape, y_train.shape

np.ptp(x_train)

# Data augmentation argument

img_data_gen_args = dict(rescale = 1./255., dtype = np.float32)

# This takes a preprocessing_function lambda as input that changes all labels into binary value (ie. either 0 or 255)
mask_data_gen_args = dict(
                     rescale = 1./255.,
                     preprocessing_function = lambda x: np.where(x>0, 255., 0).astype(x.dtype),
                     dtype = np.float32) #Binarize the output again. 


normalize_data_gen_args = dict(rescale = 1./255., dtype = np.float32)

normalize_mask_gen_args = dict(rescale = 1./255., dtype = np.float32)

# seed = 24

# image_data_generator = ImageDataGenerator(**img_data_gen_args)
# image_data_generator.fit(x_train, seed=seed)
# image_generator = image_data_generator.flow(x_train, seed=seed, batch_size = 32)
# image_data_generator = ImageDataGenerator(**mask_data_gen_args)
# image_data_generator.fit(x_test, seed = seed)
# valid_data_generator = image_data_generator.flow(x_test, seed = seed, batch_size = 32)

# mask_data_generator = ImageDataGenerator(**mask_data_gen_args)
# mask_data_generator.fit(y_train, seed=seed)
# mask_generator = mask_data_generator.flow(y_train, seed=seed, batch_size = 32)
# mask_data_generator = ImageDataGenerator(**mask_data_gen_args)
# mask_data_generator.fit(y_test, seed = seed)
# valid_mask_generator = mask_data_generator.flow(y_test, seed = seed, batch_size = 32)

# def my_image_mask_generator(image_generator, mask_generator):
#     train_generator = zip(image_generator, mask_generator)
#     for (img, mask) in train_generator:
#         yield (img, mask)

# my_generator = my_image_mask_generator(image_generator, mask_generator)
# validation_datagen = my_image_mask_generator(valid_data_generator, valid_mask_generator)

checkpointer = ModelCheckpoint(filepath="./Models", 
                               save_best_only=True,
                               save_weights_only=True
                              )
reduce_lr = ReduceLROnPlateau(monitor='val_loss',
                              mode='min',
                              patience=10,
                              min_delta=0.0001,
                              factor=0.2,
                              verbose = 1
                             )

print(type(checkpointer))

dice_loss = sm.losses.DiceLoss(class_weights=np.array([0.5, 0.5])) 
focal_loss = sm.losses.BinaryFocalLoss(gamma = 2)
total_loss = dice_loss + (1 * focal_loss)
BACKBONE = 'resnet34'
metrics = [sm.metrics.IOUScore(threshold=0.5), sm.metrics.FScore(threshold=0.5)]
adam = tf.keras.optimizers.Adam(learning_rate= 0.005)

N = x_train.shape[-1]
x_train.shape

model = sm.Unet(backbone_name='resnet34', encoder_weights=None, input_shape=(None, None, N), classes = 1)
model.compile(optimizer = adam,
              loss = 'binary_crossentropy',
              metrics = [iou_score])

process_input = sm.get_preprocessing(BACKBONE)
x_train = process_input(x_train)
x_test = process_input(x_test)

x_train = x_train.astype(np.float32)
y_train = y_train.astype(np.float32)
x_test = x_test.astype(np.float32)
y_test = y_test.astype(np.float32)

np.ptp(x_train)

model.summary()

history = model.fit(x_train, y_train, epochs = 100, batch_size = 32, validation_data = (x_test, y_test))

model.save('seg_model.h5')

from google.colab import files
files.download('seg_model.h5')